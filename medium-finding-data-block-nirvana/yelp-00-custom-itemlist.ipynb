{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Data Block Nirvana (a journey through the fastai data block API)\n",
    "\n",
    "This notebook illustrates how to create a custom `ItemList` for use in the fastai data block API.  It is heavily annotated to further aid in also understanding how all the different bits in the API interact as well as what is happening at each step and why.\n",
    "\n",
    "Please consult the [fastai docs](https://docs.fast.ai/) for installing required packages and setting up your environment to run the code below.\n",
    "\n",
    "The accompanying Medium article highlighing the data block API mechanics based on my work here can be found [here](https://medium.com/@wgilliam/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Dataset\n",
    "\n",
    "This example utilize a subset of the Yelp review dataset I've made available as part of the code repo for the purposes of illustrating how my `MixedTabularList` would work with a pandas DataFrame containing categorical, continuous, and numercalized text data.  The full dataset and documentation can be found following the links below.\n",
    "\n",
    "Available from https://www.yelp.com/dataset/download  \n",
    "Documentation here:  https://www.yelp.com/dataset/documentation/main  \n",
    "More information here:  https://www.yelp.com/dataset\n",
    "\n",
    "Unzip the `joined_sample.zip` .csv file into a `data/yelp_dataset` folder relative to this notebook and you should be good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai version: 1.0.59\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.text import *\n",
    "from fastai.text.data import _join_texts\n",
    "\n",
    "print(f'fastai version: {__version__}')  #=> I test this against 1.0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: 1\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "print(f'using GPU: {torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/yelp_dataset/joined_sample.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path('data/yelp_dataset/')\n",
    "PATH.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ItemBase subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ItemBase` defines the inputs for your custom dataset, the X and optionally y values you are going to feed into the `forward` function of your pytorch model.  Here we define what an an input item looks like (we'll let fastai infer the `ItemBase` type to use based on our target values).\n",
    "\n",
    "If your custom `ItemBase` needs to have some kind of data augmentation applied to it, you should overload the `apply_tfms` method as needed.  This method will be called you apply a `transform` block via the Data Block API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedTabularLine(TabularLine):\n",
    "    \"Item's that include both tabular data(`conts` and `cats`) and textual data (numericalized `ids`)\"\n",
    "    \n",
    "    def __init__(self, cats, conts, cat_classes, col_names, txt_ids, txt_cols, txt_string):\n",
    "        # tabular\n",
    "        super().__init__(cats, conts, cat_classes, col_names)\n",
    "\n",
    "        # add the text bits\n",
    "        self.text_ids = txt_ids\n",
    "        self.text_cols = txt_cols\n",
    "        self.text = txt_string\n",
    "        \n",
    "        # append numericalted text data to your input (represents your X values that are fed into your model)\n",
    "        # self.data = [tensor(cats), tensor(conts), tensor(txt_ids)]\n",
    "        self.data += [ np.array(txt_ids, dtype=np.int64) ]\n",
    "        self.obj = self.data\n",
    "        \n",
    "    def __str__(self):\n",
    "        res = super().__str__() + f'Text: {self.text}'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom Processor, DataBunch, and utility methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom `ItemList` is going to require a custom `PreProcessor` and a custom `DataBunch`, so we define them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedTabularProcessor(TabularProcessor):\n",
    "    \n",
    "    def __init__(self, ds:ItemList=None, procs=None, \n",
    "                 tokenizer:Tokenizer=None, chunksize:int=10000,\n",
    "                 vocab:Vocab=None, max_vocab:int=60000, min_freq:int=2):\n",
    "        #pdb.set_trace()\n",
    "        super().__init__(ds, procs)\n",
    "    \n",
    "        self.tokenizer, self.chunksize = ifnone(tokenizer, Tokenizer()), chunksize\n",
    "        \n",
    "        vocab = ifnone(vocab, ds.vocab if ds is not None else None)\n",
    "        self.vocab, self.max_vocab, self.min_freq = vocab, max_vocab, min_freq\n",
    "        \n",
    "    # process a single item in a dataset\n",
    "    # NOTE: THIS IS METHOD HAS NOT BEEN TESTED AT THIS POINT (WILL COVER IN A FUTURE ARTICLE)\n",
    "    def process_one(self, item):\n",
    "        # process tabular data (copied form tabular.data)\n",
    "        df = pd.DataFrame([item, item])\n",
    "        for proc in self.procs: proc(df, test=True)\n",
    "            \n",
    "        if len(self.cat_names) != 0:\n",
    "            codes = np.stack([c.cat.codes.values for n,c in df[self.cat_names].items()], 1).astype(np.int64) + 1\n",
    "        else: \n",
    "            codes = [[]]\n",
    "            \n",
    "        if len(self.cont_names) != 0:\n",
    "            conts = np.stack([c.astype('float32').values for n,c in df[self.cont_names].items()], 1)\n",
    "        else: \n",
    "            conts = [[]]\n",
    "            \n",
    "        classes = None\n",
    "        col_names = list(df[self.cat_names].columns.values) + list(df[self.cont_names].columns.values)\n",
    "        \n",
    "        # process textual data\n",
    "        if len(self.text_cols) != 0:\n",
    "            txt = _join_texts(df[self.text_cols].values, (len(self.text_cols) > 1))\n",
    "            txt_toks = self.tokenizer._process_all_1(txt)[0]\n",
    "            text_ids = np.array(self.vocab.numericalize(txt_toks), dtype=np.int64)\n",
    "        else:\n",
    "            txt_toks, text_ids = None, [[]]\n",
    "            \n",
    "        # return ItemBase\n",
    "        return MixedTabularLine(codes[0], conts[0], classes, col_names, text_ids, self.txt_cols, txt_toks)\n",
    "    \n",
    "    # processes the entire dataset\n",
    "    def process(self, ds):\n",
    "        #pdb.set_trace()\n",
    "        # process tabular data and then set \"preprocessed=False\" since we still have text data possibly\n",
    "        super().process(ds)\n",
    "        ds.preprocessed = False\n",
    "        \n",
    "        # process text data from column(s) containing text\n",
    "        if len(ds.text_cols) != 0:\n",
    "            texts = _join_texts(ds.inner_df[ds.text_cols].values, (len(ds.text_cols) > 1))\n",
    "\n",
    "            # tokenize (set = .text)\n",
    "            tokens = []\n",
    "            for i in progress_bar(range(0, len(ds), self.chunksize), leave=False):\n",
    "                tokens += self.tokenizer.process_all(texts[i:i+self.chunksize])\n",
    "            ds.text = tokens\n",
    "\n",
    "            # set/build vocab\n",
    "            if self.vocab is None: self.vocab = Vocab.create(ds.text, self.max_vocab, self.min_freq)\n",
    "            ds.vocab = self.vocab\n",
    "            ds.text_ids = [ np.array(self.vocab.numericalize(toks), dtype=np.int64) for toks in ds.text ]\n",
    "        else:\n",
    "            ds.text, ds.vocab, ds.text_ids = None, None, []\n",
    "            \n",
    "        ds.preprocessed = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to the \"fasta.text.data.pad_collate\" except that it is designed to work with MixedTabularLine items,\n",
    "# where the final thing in an item is the numericalized text ids.\n",
    "# we need a collate function to ensure a square matrix with the text ids, which will be of variable length.\n",
    "def mixed_tabular_pad_collate(samples:BatchSamples, \n",
    "                              pad_idx:int=1, pad_first:bool=True) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding.\"\n",
    "\n",
    "    samples = to_data(samples)\n",
    "    max_len = max([len(s[0][-1]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "   \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res[i,-len(s[0][-1]):] = LongTensor(s[0][-1])\n",
    "        else:         \n",
    "            res[i,:len(s[0][-1]):] = LongTensor(s[0][-1])\n",
    "            \n",
    "        # replace the text_ids array (the last thing in the inputs) with the padded tensor matrix\n",
    "        s[0][-1] = res[i]\n",
    "              \n",
    "    # for the inputs, return a list containing 3 elements: a list of cats, a list of conts, and a list of text_ids\n",
    "    return [x for x in zip(*[s[0] for s in samples])], tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each \"ds\" is of type LabelList(Dataset)\n",
    "class MixedTabularDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs=64, \n",
    "               pad_idx=1, pad_first=True, no_check:bool=False, **kwargs) -> DataBunch:\n",
    "        \n",
    "        # only thing we're doing here is setting the collate_fn = to our new \"pad_collate\" method above\n",
    "        collate_fn = partial(mixed_tabular_pad_collate, pad_idx=pad_idx, pad_first=pad_first)\n",
    "        \n",
    "        kwargs['collate_fn'] = collate_fn\n",
    "        kwargs['num_workers'] = 1\n",
    "        return super().create(train_ds, valid_ds, test_ds, path=path, bs=bs, no_check=no_check, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ItemList subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `ItemList` consists of a set of `ItemBase` objects. Once created, you can use any of splitting or labeling methods prior to creating a `DataBunch` for training.\n",
    "\n",
    "You'll likely want to set the following three class variables to something specific to your situation:\n",
    "\n",
    "**`_bunch`**:  \n",
    "The name of the class used to create a `DataBunch`.  `TabularList` uses the default `DataBunch` as is and so does not set this variable. We create a custom `DataBunch` here because we need to add padding to the column with the text ids in order to ensure a square matrix per batch before integrating the text bits with the tabular.\n",
    "\n",
    "When you call `databunch()` via the Data Block API, `_bunch.create` will be called passing in the datasets (training, validation and optionally test) defined by your `ItemLists` and returning a set of `DataLoader`s in a `DataBunch` for training.\n",
    "\n",
    "**`_processor`**:  \n",
    "A class or list of classes of type `PreProcessor` that will be used to create the default processor for this `ItemList`.\n",
    "\n",
    "The processors are **called at the end of the labelling** to apply some kind of function on your items. The **default processor of the inputs** can be overriden by passing a `processor` in the kwargs when creating the `ItemList`, the **default processor of the targets** can be overriden by passing a `processor` in the kwargs of the labelling function.\n",
    "\n",
    "Processors are useful for pre-processing data, and **you also need to save any computed state required for future datasets when `data.export()` is called.**\n",
    "\n",
    "**`_item_cls`**:   \n",
    "The name of the class that will be used to create the \"items\" by default.\n",
    "\n",
    "**`_label_cls`**:   \n",
    "The name of the class that will be used to create the labels by default. (**If this variable is set to None, the label class will be guessed** between `CategoryList`, `MultiCategoryList` and `FloatList` depending on the type of the first item. Since we are creating a custom `ItemList` with a very distinct signature, we want to set it to that class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedTabularList(TabularList):\n",
    "    \"A custom `ItemList` that merges tabular data along with textual data\"\n",
    "    \n",
    "    _item_cls = MixedTabularLine\n",
    "    _processor = MixedTabularProcessor\n",
    "    _bunch = MixedTabularDataBunch\n",
    "    \n",
    "    def __init__(self, items:Iterator, cat_names:OptStrList=None, cont_names:OptStrList=None, \n",
    "                 text_cols=None, vocab:Vocab=None, pad_idx:int=1, \n",
    "                 procs=None, **kwargs) -> 'MixedTabularList':\n",
    "        #pdb.set_trace()\n",
    "        super().__init__(items, cat_names, cont_names, procs, **kwargs)\n",
    "        \n",
    "        self.cols = [] if cat_names == None else cat_names.copy()\n",
    "        if cont_names: self.cols += cont_names.copy()\n",
    "        if txt_cols: self.cols += text_cols.copy()\n",
    "        \n",
    "        self.text_cols, self.vocab, self.pad_idx = text_cols, vocab, pad_idx\n",
    "        \n",
    "        # add any ItemList state into \"copy_new\" that needs to be copied each time \"new()\" is called; \n",
    "        # your ItemList acts as a prototype for training, validation, and/or test ItemList instances that\n",
    "        # are created via ItemList.new()\n",
    "        self.copy_new += ['text_cols', 'vocab', 'pad_idx']\n",
    "        \n",
    "        self.preprocessed = False\n",
    "        \n",
    "    # defines how to construct an ItemBase from the data in the ItemList.items array\n",
    "    def get(self, i):\n",
    "        if not self.preprocessed: \n",
    "            return self.inner_df.iloc[i][self.cols] if hasattr(self, 'inner_df') else self.items[i]\n",
    "        \n",
    "        codes = [] if self.codes is None else self.codes[i]\n",
    "        conts = [] if self.conts is None else self.conts[i]\n",
    "        text_ids = [] if self.text_ids is None else self.text_ids[i]\n",
    "        text_string = None if self.text_ids is None else self.vocab.textify(self.text_ids[i])\n",
    "        \n",
    "        return self._item_cls(codes, conts, self.classes, self.col_names, text_ids, self.text_cols, text_string)\n",
    "    \n",
    "    # this is the method that is called in data.show_batch(), learn.predict() or learn.show_results() \n",
    "    # to transform a pytorch tensor back in an ItemBase. \n",
    "    # in a way, it does the opposite of calling ItemBase.data. It should take a tensor t and return \n",
    "    # the same king of thing as the get method.\n",
    "    def reconstruct(self, t:Tensor):\n",
    "        return self._item_cls(t[0], t[1], self.classes, self.col_names, \n",
    "                              t[2], self.text_cols, self.vocab.textify(t[2]))\n",
    "    \n",
    "    # tells fastai how to display a custom ItemBase when data.show_batch() is called\n",
    "    def show_xys(self, xs, ys) -> None:\n",
    "        \"Show the `xs` (inputs) and `ys` (targets).\"\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        # show tabular\n",
    "        display(HTML('TABULAR:<br>'))\n",
    "        super().show_xys(xs, ys)\n",
    "        \n",
    "        # show text\n",
    "        items = [['text_data', 'target']]\n",
    "        for i, (x,y) in enumerate(zip(xs,ys)):\n",
    "            res = []\n",
    "            res += [' '.join([ f'{tok}({self.vocab.stoi[tok]})' \n",
    "                              for tok in x.text.split() if (not self.vocab.stoi[tok] == self.pad_idx) ])]\n",
    "                \n",
    "            res += [str(y)]\n",
    "            items.append(res)\n",
    "            \n",
    "        col_widths = [90, 1]\n",
    "        \n",
    "        display(HTML('TEXT:<br>'))\n",
    "        display(HTML(text2html_table(items)))\n",
    "        \n",
    "    # tells fastai how to display a custom ItemBase when learn.show_results() is called\n",
    "    def show_xyzs(self, xs, ys, zs):\n",
    "        \"Show `xs` (inputs), `ys` (targets) and `zs` (predictions).\"\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        # show tabular\n",
    "        super().show_xyzs(xs, ys, zs)\n",
    "        \n",
    "        # show text\n",
    "        items = [['text_data','target', 'prediction']]\n",
    "        for i, (x,y,z) in enumerate(zip(xs,ys,zs)):\n",
    "            res = []\n",
    "            res += [' '.join([ f'{tok}({self.vocab.stoi[tok]})'\n",
    "                              for tok in x.text.split() if (not self.vocab.stoi[tok] == self.pad_idx) ])]\n",
    "                \n",
    "            res += [str(y),str(z)]\n",
    "            items.append(res)\n",
    "            \n",
    "        col_widths = [90, 1, 1]\n",
    "        display(HTML('<br>' + text2html_table(items)))\n",
    "    \n",
    "        \n",
    "    @classmethod\n",
    "    def from_df(cls, df:DataFrame, cat_names:OptStrList=None, cont_names:OptStrList=None, \n",
    "                text_cols=None, vocab=None, procs=None, **kwargs) -> 'ItemList':\n",
    "        \n",
    "        return cls(items=range(len(df)), cat_names=cat_names, cont_names=cont_names, \n",
    "                   text_cols=text_cols, vocab=vocab, procs=procs, inner_df=df, **kwargs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch joined yelp reviews (includes busines and user info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_average_stars</th>\n",
       "      <th>...</th>\n",
       "      <th>business_hours</th>\n",
       "      <th>business_is_open</th>\n",
       "      <th>business_latitude</th>\n",
       "      <th>business_longitude</th>\n",
       "      <th>business_name</th>\n",
       "      <th>business_neighborhood</th>\n",
       "      <th>business_postal_code</th>\n",
       "      <th>business_review_count</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>business_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8jpIK1WHmzzbXPaK51GenQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-08-08</td>\n",
       "      <td>3</td>\n",
       "      <td>W7wcVRiw5T8TMrmGnxPsxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>I've been here at least 10 times ... I like it...</td>\n",
       "      <td>1</td>\n",
       "      <td>g6gTSnUKZIxLZPQVrFKscw</td>\n",
       "      <td>4.14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Tuesday': '6:30-14:30', 'Wednesday': '6:30-1...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.320994</td>\n",
       "      <td>-111.912682</td>\n",
       "      <td>Dessie's Cafe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85226</td>\n",
       "      <td>67</td>\n",
       "      <td>3.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wH4Q0y8C-lkq21yf4WWedw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>emypFL3PJjQBcllPZw_d5A</td>\n",
       "      <td>5</td>\n",
       "      <td>Although I had heard of Nekter, mainly from se...</td>\n",
       "      <td>2</td>\n",
       "      <td>LAEJWZSvzsfWJ686VOaQig</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '6:30-20:0', 'Tuesday': '6:30-20:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.580474</td>\n",
       "      <td>-111.881062</td>\n",
       "      <td>Nekter Juice Bar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85260</td>\n",
       "      <td>59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cRMC2eQ9CP6ivhEY8EdaGg</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-09-13</td>\n",
       "      <td>0</td>\n",
       "      <td>5X5ISEAp6HFTpMd_wlq_9w</td>\n",
       "      <td>3</td>\n",
       "      <td>Last week I met up with a highschool friend fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>TwilnpgwW43r9-O2AS4PDQ</td>\n",
       "      <td>3.14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...</td>\n",
       "      <td>0</td>\n",
       "      <td>43.664193</td>\n",
       "      <td>-79.380196</td>\n",
       "      <td>Chino Locos</td>\n",
       "      <td>Church-Wellesley Village</td>\n",
       "      <td>M4Y 2C5</td>\n",
       "      <td>34</td>\n",
       "      <td>3.5</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zunMkZ4U2eVojempQtLngg</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>0</td>\n",
       "      <td>OGekU1U_wWgV--zL2gEgYw</td>\n",
       "      <td>4</td>\n",
       "      <td>A friend and I were driving by and decided to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>eITkQlKYsYqOBASP-QS0iQ</td>\n",
       "      <td>3.72</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '11:0-1:0', 'Tuesday': '11:0-1:0', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.639158</td>\n",
       "      <td>-112.185110</td>\n",
       "      <td>The Australian AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85308</td>\n",
       "      <td>26</td>\n",
       "      <td>2.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1vLf-v7foAu3tJ7vAEoKdA</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-26</td>\n",
       "      <td>1</td>\n",
       "      <td>tTe2cLFmpkLop3wKcT0Zgw</td>\n",
       "      <td>5</td>\n",
       "      <td>Our Bulldog LOVES this place and so do we! Won...</td>\n",
       "      <td>0</td>\n",
       "      <td>l3okl_UjyNdqRKAzYGdWaA</td>\n",
       "      <td>2.95</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Monday': '7:30-19:0', 'Tuesday': '7:30-19:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.582848</td>\n",
       "      <td>-111.929296</td>\n",
       "      <td>Lori's Grooming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85254</td>\n",
       "      <td>148</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool        date  funny               review_id  \\\n",
       "0  8jpIK1WHmzzbXPaK51GenQ     1  2012-08-08      3  W7wcVRiw5T8TMrmGnxPsxQ   \n",
       "1  wH4Q0y8C-lkq21yf4WWedw     0  2015-01-31      0  emypFL3PJjQBcllPZw_d5A   \n",
       "2  cRMC2eQ9CP6ivhEY8EdaGg     1  2010-09-13      0  5X5ISEAp6HFTpMd_wlq_9w   \n",
       "3  zunMkZ4U2eVojempQtLngg     1  2014-03-07      0  OGekU1U_wWgV--zL2gEgYw   \n",
       "4  1vLf-v7foAu3tJ7vAEoKdA     0  2014-11-26      1  tTe2cLFmpkLop3wKcT0Zgw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      4  I've been here at least 10 times ... I like it...       1   \n",
       "1      5  Although I had heard of Nekter, mainly from se...       2   \n",
       "2      3  Last week I met up with a highschool friend fo...       1   \n",
       "3      4  A friend and I were driving by and decided to ...       1   \n",
       "4      5  Our Bulldog LOVES this place and so do we! Won...       0   \n",
       "\n",
       "                  user_id  user_average_stars  ...  \\\n",
       "0  g6gTSnUKZIxLZPQVrFKscw                4.14  ...   \n",
       "1  LAEJWZSvzsfWJ686VOaQig                5.00  ...   \n",
       "2  TwilnpgwW43r9-O2AS4PDQ                3.14  ...   \n",
       "3  eITkQlKYsYqOBASP-QS0iQ                3.72  ...   \n",
       "4  l3okl_UjyNdqRKAzYGdWaA                2.95  ...   \n",
       "\n",
       "                                      business_hours  business_is_open  \\\n",
       "0  {'Tuesday': '6:30-14:30', 'Wednesday': '6:30-1...                 0   \n",
       "1  {'Monday': '6:30-20:0', 'Tuesday': '6:30-20:0'...                 1   \n",
       "2  {'Monday': '12:0-21:0', 'Tuesday': '12:0-21:0'...                 0   \n",
       "3  {'Monday': '11:0-1:0', 'Tuesday': '11:0-1:0', ...                 0   \n",
       "4  {'Monday': '7:30-19:0', 'Tuesday': '7:30-19:0'...                 1   \n",
       "\n",
       "   business_latitude  business_longitude      business_name  \\\n",
       "0          33.320994         -111.912682      Dessie's Cafe   \n",
       "1          33.580474         -111.881062   Nekter Juice Bar   \n",
       "2          43.664193          -79.380196        Chino Locos   \n",
       "3          33.639158         -112.185110  The Australian AZ   \n",
       "4          33.582848         -111.929296    Lori's Grooming   \n",
       "\n",
       "      business_neighborhood  business_postal_code  business_review_count  \\\n",
       "0                       NaN                 85226                     67   \n",
       "1                       NaN                 85260                     59   \n",
       "2  Church-Wellesley Village               M4Y 2C5                     34   \n",
       "3                       NaN                 85308                     26   \n",
       "4                       NaN                 85254                    148   \n",
       "\n",
       "   business_stars  business_state  \n",
       "0             3.5              AZ  \n",
       "1             4.0              AZ  \n",
       "2             3.5              ON  \n",
       "3             2.5              AZ  \n",
       "4             5.0              AZ  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.558300</td>\n",
       "      <td>1.975650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>4.304098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.721300</td>\n",
       "      <td>1.455211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>3.678959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_average_stars</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.739603</td>\n",
       "      <td>0.802876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_cool</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>37.518800</td>\n",
       "      <td>308.275809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_cute</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.695100</td>\n",
       "      <td>31.125260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_funny</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>37.518800</td>\n",
       "      <td>308.275809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_hot</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>25.273900</td>\n",
       "      <td>240.013823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9259.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_list</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.222300</td>\n",
       "      <td>27.808089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2259.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_more</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.228200</td>\n",
       "      <td>42.844896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3574.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_note</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>17.064600</td>\n",
       "      <td>121.196187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4899.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_photos</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>14.587600</td>\n",
       "      <td>189.604656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10820.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_plain</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>42.968400</td>\n",
       "      <td>351.849322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11741.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_profile</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.686700</td>\n",
       "      <td>65.375613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5659.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_compliment_writer</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>14.075400</td>\n",
       "      <td>117.437035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5668.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_cool</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>228.383500</td>\n",
       "      <td>2234.239711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>86136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_fans</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>11.817500</td>\n",
       "      <td>53.572480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_funny</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>153.556100</td>\n",
       "      <td>1732.724499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>83218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_review_count</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>121.117200</td>\n",
       "      <td>348.622585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>9278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_useful</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>295.013100</td>\n",
       "      <td>2471.007897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>91508.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_is_open</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.870600</td>\n",
       "      <td>0.335659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_latitude</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>37.255640</td>\n",
       "      <td>4.180602</td>\n",
       "      <td>-34.513715</td>\n",
       "      <td>33.611538</td>\n",
       "      <td>36.106370</td>\n",
       "      <td>40.448994</td>\n",
       "      <td>54.486937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_longitude</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>-102.726057</td>\n",
       "      <td>16.112593</td>\n",
       "      <td>-122.822578</td>\n",
       "      <td>-115.154014</td>\n",
       "      <td>-111.978453</td>\n",
       "      <td>-80.993009</td>\n",
       "      <td>112.092039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_review_count</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>368.128600</td>\n",
       "      <td>785.565596</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>7968.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_stars</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.736500</td>\n",
       "      <td>0.752479</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count        mean          std         min  \\\n",
       "cool                     10000.0    0.558300     1.975650    0.000000   \n",
       "funny                    10000.0    0.534500     4.304098    0.000000   \n",
       "stars                    10000.0    3.721300     1.455211    1.000000   \n",
       "useful                   10000.0    1.368800     3.678959    0.000000   \n",
       "user_average_stars       10000.0    3.739603     0.802876    1.000000   \n",
       "user_compliment_cool     10000.0   37.518800   308.275809    0.000000   \n",
       "user_compliment_cute     10000.0    1.695100    31.125260    0.000000   \n",
       "user_compliment_funny    10000.0   37.518800   308.275809    0.000000   \n",
       "user_compliment_hot      10000.0   25.273900   240.013823    0.000000   \n",
       "user_compliment_list     10000.0    1.222300    27.808089    0.000000   \n",
       "user_compliment_more     10000.0    3.228200    42.844896    0.000000   \n",
       "user_compliment_note     10000.0   17.064600   121.196187    0.000000   \n",
       "user_compliment_photos   10000.0   14.587600   189.604656    0.000000   \n",
       "user_compliment_plain    10000.0   42.968400   351.849322    0.000000   \n",
       "user_compliment_profile  10000.0    2.686700    65.375613    0.000000   \n",
       "user_compliment_writer   10000.0   14.075400   117.437035    0.000000   \n",
       "user_cool                10000.0  228.383500  2234.239711    0.000000   \n",
       "user_fans                10000.0   11.817500    53.572480    0.000000   \n",
       "user_funny               10000.0  153.556100  1732.724499    0.000000   \n",
       "user_review_count        10000.0  121.117200   348.622585    0.000000   \n",
       "user_useful              10000.0  295.013100  2471.007897    0.000000   \n",
       "business_is_open         10000.0    0.870600     0.335659    0.000000   \n",
       "business_latitude        10000.0   37.255640     4.180602  -34.513715   \n",
       "business_longitude       10000.0 -102.726057    16.112593 -122.822578   \n",
       "business_review_count    10000.0  368.128600   785.565596    3.000000   \n",
       "business_stars           10000.0    3.736500     0.752479    1.000000   \n",
       "\n",
       "                                25%         50%         75%           max  \n",
       "cool                       0.000000    0.000000    0.000000     70.000000  \n",
       "funny                      0.000000    0.000000    0.000000    388.000000  \n",
       "stars                      3.000000    4.000000    5.000000      5.000000  \n",
       "useful                     0.000000    0.000000    2.000000    212.000000  \n",
       "user_average_stars         3.400000    3.810000    4.200000      5.000000  \n",
       "user_compliment_cool       0.000000    0.000000    2.000000  13014.000000  \n",
       "user_compliment_cute       0.000000    0.000000    0.000000   2250.000000  \n",
       "user_compliment_funny      0.000000    0.000000    2.000000  13014.000000  \n",
       "user_compliment_hot        0.000000    0.000000    1.000000   9259.000000  \n",
       "user_compliment_list       0.000000    0.000000    0.000000   2259.000000  \n",
       "user_compliment_more       0.000000    0.000000    1.000000   3574.000000  \n",
       "user_compliment_note       0.000000    0.000000    2.000000   4899.000000  \n",
       "user_compliment_photos     0.000000    0.000000    0.000000  10820.000000  \n",
       "user_compliment_plain      0.000000    0.000000    3.000000  11741.000000  \n",
       "user_compliment_profile    0.000000    0.000000    0.000000   5659.000000  \n",
       "user_compliment_writer     0.000000    0.000000    2.000000   5668.000000  \n",
       "user_cool                  0.000000    0.000000    5.000000  86136.000000  \n",
       "user_fans                  0.000000    0.000000    4.000000   1394.000000  \n",
       "user_funny                 0.000000    0.000000    6.000000  83218.000000  \n",
       "user_review_count          7.000000   23.000000   93.250000   9278.000000  \n",
       "user_useful                0.000000    3.000000   26.000000  91508.000000  \n",
       "business_is_open           1.000000    1.000000    1.000000      1.000000  \n",
       "business_latitude         33.611538   36.106370   40.448994     54.486937  \n",
       "business_longitude      -115.154014 -111.978453  -80.993009    112.092039  \n",
       "business_review_count     35.000000  111.000000  339.000000   7968.000000  \n",
       "business_stars             3.500000    4.000000    4.000000      5.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_df = pd.read_csv(PATH/'joined_sample.csv', index_col=None)\n",
    "\n",
    "display(len(joined_df))\n",
    "display(joined_df.head())\n",
    "display(joined_df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use and test our MixedTabularList ItemList with the Data Block API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['business_id', 'user_id', 'business_stars', 'business_postal_code', 'business_state']\n",
    "cont_cols = ['useful', 'user_average_stars', 'user_review_count', 'business_review_count']\n",
    "txt_cols = ['text']\n",
    "\n",
    "dep_var = ['stars']\n",
    "\n",
    "procs = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define the source of your inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = MixedTabularList.from_df(joined_df, cat_cols, cont_cols, txt_cols, vocab=None, procs=procs, path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATS:\n",
      "['business_id', 'user_id', 'business_stars', 'business_postal_code', 'business_state']\n",
      "CONTS:\n",
      "['useful', 'user_average_stars', 'user_review_count', 'business_review_count']\n",
      "TEXT COLS:\n",
      "['text']\n",
      "PROCS:\n",
      "[<class 'fastai.tabular.transform.FillMissing'>, <class 'fastai.tabular.transform.Categorify'>, <class 'fastai.tabular.transform.Normalize'>]\n",
      "\n",
      "business_id                                         8jpIK1WHmzzbXPaK51GenQ\n",
      "user_id                                             g6gTSnUKZIxLZPQVrFKscw\n",
      "business_stars                                                         3.5\n",
      "business_postal_code                                                 85226\n",
      "business_state                                                          AZ\n",
      "useful                                                                   1\n",
      "user_average_stars                                                    4.14\n",
      "user_review_count                                                       26\n",
      "business_review_count                                                   67\n",
      "text                     I've been here at least 10 times ... I like it...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f'CATS:\\n{il.cat_names}')\n",
    "print(f'CONTS:\\n{il.cont_names}')\n",
    "print(f'TEXT COLS:\\n{il.text_cols}')\n",
    "print(f'PROCS:\\n{il.procs}')\n",
    "print('')\n",
    "print(il.get(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Split your dataset into training and validation `ItemList`s**\n",
    "\n",
    "This is going to trigger the `ItemList.new()` method getting called for each `ItemList` it needs to create (e.g., train, validation).  Here it will be called 2x, once to create the training dataset and then to create the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ils = il.split_by_rand_pct(valid_pct=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1000, PosixPath('data/yelp_dataset'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ils.train), len(ils.valid), ils.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Add your labels (your targets or \"y\" values)**\n",
    "\n",
    "This will grab the targets (the \"y\") for each `ItemList` in your `ItemLists` object (e.g, `.train`, `.valid`) and build a `LabelList(Dataset)` for each accordingly that is then combined in and returned in a `LabelLists` object.\n",
    "\n",
    "You'll notice that the processor is created 1x but that .process is called 2x.  *Why?* So that the preprocessing defined by the training data is applied to the validation and optionally the test data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = ils.label_from_df(dep_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fastai.data_block.LabelLists, fastai.data_block.LabelList, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ll), type(ll.train), len(ll.lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelList (9000 items)\n",
       "x: MixedTabularList\n",
       "business_id wH4Q0y8C-lkq21yf4WWedw; user_id LAEJWZSvzsfWJ686VOaQig; business_stars 4.0; business_postal_code 85260; business_state AZ; useful 0.1650; user_average_stars 1.5663; user_review_count -0.3320; business_review_count -0.3931; Text: xxbos xxmaj although i had heard of xxmaj xxunk , mainly from seeing it xxunk in health conscious friends ' xxup ig posts , i had never tried it . xxmaj this location is rather new and conveniently located to me , so i gave it a try . xxmaj it 's xxup amazing ! xxmaj staff and the customer service they provide are phenomenal , and the having tried the juices ( fresh cold - pressed to order ) , smoothies & acai bowls , i 'm in love . xxmaj my 3 year old daughter even enjoyed the xxmaj pink xxmaj flamingo smoothie ( which they sell in kids size for little bellies ) . xxmaj they also sell pre - made detox and protein drinks in a small ready - to - go cooler area by the check out , although i have n't tried these yet . xxmaj and there 's a limited selection of granola & protein bars plus some kale chips in terms of snacks . \n",
       " \n",
       "  xxmaj if you 're craving a treat , treat the xxmaj acai xxmaj berry or xxup pb xxmaj bowl which is like sorbet almost and comes topped with fresh berries , banana & granola , making it a perfect breakfast or afternoon snack . xxmaj little less filling but equally delicious are the smoothies . xxmaj or get refreshed and xxunk with the juices without the hassle of cleaning your xxunk at home ( cuz anyone who juices knows it 's a pain in who - know - where ) ! xxmaj drinks and bowls run around $ 5 - 7 ish and there 's options to add protein , extra greens , or customize anything . \n",
       " \n",
       "  xxmaj for me , this place has been a great place to stop by with my daughter after preschool and / or a good way to ensure i do n't go grocery shopping at xxmaj sprouts ( same parking lot ) hungry :),business_id cRMC2eQ9CP6ivhEY8EdaGg; user_id TwilnpgwW43r9-O2AS4PDQ; business_stars 3.5; business_postal_code M4Y 2C5; business_state ON; useful -0.0993; user_average_stars -0.7442; user_review_count -0.0778; business_review_count -0.4249; Text: xxbos xxmaj last week i met up with a highschool friend for the first time after highschool graduation for burritos at xxmaj chino xxmaj locos : xxmaj dos xxmaj locos , the xxmaj church street venue . i 'd never been to this xxmaj mexican fast food joint before , but i heard great things about their burritos , which is the only thing they make and serve . i was glad to find a spotless and nicely lit dining area and knowledgeable and friendly burrito cooks / cashiers ( they 're the same person ) . \n",
       " \n",
       "  xxmaj their menu is rather limited , but that 's not necessarily a bad thing because what they offer is actually quite good and there 's something for everyone , particularly for me , the vegan : they have 1 vegetarian burrito with cheese and sour cream , but they also have 1 vegan burrito that is sans dairy and meat - \" the juicy vegan . \" xxmaj the vegan burrito was so good ... priced at $ 7.99 before taxes , it 's actually the most expensive burrito on their menu ! ! ! xxmaj the burrito consists of pressed tofu , eggplant , shitake mushrooms , glass noodles , guacamole , tomatoes , edamame beans , red onions , black beans , green peppers , cilantro , chipotle sauce , rustic rice ... all wrapped in a whole wheat wrap . ( xxmaj you have the option between white and whole wheat ) . xxmaj the cooks told me that they made their own guacamole at the restaurant , and to me , anything made from scratch is a big bonus ! xxmaj one set back to my burrito experience was the hot sauce ... it was n't hot enough , even though i 'd asked for extra hot xxup -i xxup love spicy food . \n",
       " \n",
       "  xxmaj you should check this place out if you like xxmaj asian food and xxmaj mexican food ... the xxmaj asian - xxmaj mexican influence on my burrito made it a crazy dining experience for my taste buds ... those xxmaj chino xxmaj locos xxunk ! ( xxmaj translation , xxmaj chino xxmaj locos = \" crazy xxmaj asians \" in xxmaj spanish ) .,business_id 1vLf-v7foAu3tJ7vAEoKdA; user_id l3okl_UjyNdqRKAzYGdWaA; business_stars 5.0; business_postal_code 85254; business_state AZ; useful -0.3635; user_average_stars -0.9803; user_review_count -0.2949; business_review_count -0.2798; Text: xxbos xxmaj our xxmaj bulldog xxup loves this place and so do we ! xxmaj wonderful husband and wife team that are true dog lovers . xxmaj ca n't speak highly enough of them . xxmaj only place we really trust to leave our dog . xxmaj best advertisement though is that our dog xxunk us to their door after we get out of the car ! xxup :d,business_id bWucOPNoIjd8ECdiDyVq9Q; user_id Ck3-SikwEb0U9G7RKh-O_w; business_stars 4.5; business_postal_code 85225; business_state AZ; useful -0.3635; user_average_stars -0.7815; user_review_count -0.3035; business_review_count 0.0014; Text: xxbos xxmaj it was n't bad xxrep 4 . but . we have been going to xxmaj yao for several years and it 's only 2 miles away from xxmaj singing xxmaj panda . xxmaj every time i find a new xxmaj chinese place , i am disappointed ... in comparison to xxmaj yao . xxmaj singing xxmaj panda was the same feeling . xxmaj plus , we spent 30 % more money for 30 % less food than we get at xxmaj yao xxup and it was n't nearly as tasty as xxmaj yao .,business_id tIvDO_1WNbb6UAifErQ-Ug; user_id cwZ64E_XT92XesWkQh85YQ; business_stars 4.0; business_postal_code M5A 3C4; business_state ON; useful -0.3635; user_average_stars 1.3799; user_review_count 0.1336; business_review_count -0.2722; Text: xxbos xxmaj we were at the xxmaj distillery district in xxmaj toronto . xxmaj the xxmaj balzac has one of the most interesting the decor and spaces for a coffee shop . a xxunk ceiling , funky decor and friendly staff makes for a great place to enjoy the well - made xxmaj flat xxmaj white i ordered . \n",
       " \n",
       "  i would recommend searching out xxmaj balzac xxmaj coffee shop if you 're in xxmaj toronto .\n",
       "y: CategoryList\n",
       "5,3,5,2,5\n",
       "Path: data/yelp_dataset"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MixedTabularLine business_id wH4Q0y8C-lkq21yf4WWedw; user_id LAEJWZSvzsfWJ686VOaQig; business_stars 4.0; business_postal_code 85260; business_state AZ; useful 0.1650; user_average_stars 1.5663; user_review_count -0.3320; business_review_count -0.3931; Text: xxbos xxmaj although i had heard of xxmaj xxunk , mainly from seeing it xxunk in health conscious friends ' xxup ig posts , i had never tried it . xxmaj this location is rather new and conveniently located to me , so i gave it a try . xxmaj it 's xxup amazing ! xxmaj staff and the customer service they provide are phenomenal , and the having tried the juices ( fresh cold - pressed to order ) , smoothies & acai bowls , i 'm in love . xxmaj my 3 year old daughter even enjoyed the xxmaj pink xxmaj flamingo smoothie ( which they sell in kids size for little bellies ) . xxmaj they also sell pre - made detox and protein drinks in a small ready - to - go cooler area by the check out , although i have n't tried these yet . xxmaj and there 's a limited selection of granola & protein bars plus some kale chips in terms of snacks . \n",
       "  \n",
       "   xxmaj if you 're craving a treat , treat the xxmaj acai xxmaj berry or xxup pb xxmaj bowl which is like sorbet almost and comes topped with fresh berries , banana & granola , making it a perfect breakfast or afternoon snack . xxmaj little less filling but equally delicious are the smoothies . xxmaj or get refreshed and xxunk with the juices without the hassle of cleaning your xxunk at home ( cuz anyone who juices knows it 's a pain in who - know - where ) ! xxmaj drinks and bowls run around $ 5 - 7 ish and there 's options to add protein , extra greens , or customize anything . \n",
       "  \n",
       "   xxmaj for me , this place has been a great place to stop by with my daughter after preschool and / or a good way to ensure i do n't go grocery shopping at xxmaj sprouts ( same parking lot ) hungry :),\n",
       " Category 5,\n",
       " array([7170, 3012,    7,  311,    2]),\n",
       " ['business_id',\n",
       "  'user_id',\n",
       "  'business_stars',\n",
       "  'business_postal_code',\n",
       "  'business_state'],\n",
       " array([  2,   5, 546,  13, ..., 216,  52, 793, 616]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train.x[0], ll.train.y[0], ll.train.x.codes[0], ll.train.x.cat_names, ll.train.x.text_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15256, 15256)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ll.train.x.vocab.itos), len(ll.valid.x.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Build your DataBunch**\n",
    "\n",
    "We're skilling steps 4 (add a test dataset) and 5 (apply data augmentation) since we have neither a test set or any transforms we need to apply to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 64, 64, 64, torch.Size([64]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bunch = ll.databunch(bs=64)\n",
    "b = data_bunch.one_batch()\n",
    "len(b), len(b[0]), len(b[0][0]), len(b[0][1]), len(b[0][1]), b[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len(b) = 2`:  the inputs and the targets\n",
    "\n",
    "`len(b[0]) = 3`: the three things in the input (cats, conts, text_ids)\n",
    "\n",
    "`len(b[0][0|1|2|]) = 64`: there are 64 of each of the 3 things (so there is a list 64 categorical tensors followed by a list of 64 continuous tensors that is followed by a list of 64 text tensors)\n",
    "\n",
    "The shape length of the categorical and continuous tensors are the same for every batch, whereas the shape of the numericalized token ids will be the same *per* batch thanks to the `mixed_tabular_pad_collate` function above.  This fulfills the requirement that each of the inputs be a squared matrix per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4916,  622,    7,  243,    2]),\n",
       " tensor([-0.0993,  0.7340, -0.3377, -0.3397]),\n",
       " tensor([    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     2,    13,    35,  1398,\n",
       "             5,   800,   132,     5,  1337,   761,   222,    64,    12,    11,\n",
       "            13,   341,    15,   176,    14,   283,   340,   218,   417,     9,\n",
       "             5,    23,   120,  1875,    19,    24,    10,   113,   693,    41,\n",
       "            50,   121,    11,  5879,     9,     5,    72,    13,   127,   340,\n",
       "            29,  4469,   839,    49,    12,   136,   198,    49,    94,  1629,\n",
       "            79,    10,   264,    11,  1105,     9,     5,   340,    10,   264,\n",
       "           659,    19,    50,   682,    12,   104,  2033,    12,   104,  3208,\n",
       "            12,    11,   289,    18,  8995,     9,     5,    48,    41,   183,\n",
       "           158,  1740,    12,   774,  1924,    12,   435,  4087,    12,   326,\n",
       "          1160,    12, 13096,  1924,    54,     5,  7228,    11,     5,  8996,\n",
       "          1339,    12,   578,    52,    11,    14,   395,    18,  5892,     9,\n",
       "             5,   384,  1783,  5236,  2994,    11,    55,  8458,   323,    55,\n",
       "          2076,   384,   843,  2035,    12,  7882,    11,  8478,     9,    13,\n",
       "            16,   234,    15,   177,   355,    78,  3498,    18,  1935,    12,\n",
       "           843,   491,  1935,     9,     5,    10,   264,   659,    19,    33,\n",
       "           321,    31,    29,    59,    35,    17,  1884,    57,   104,     9,\n",
       "            13,    81,   126,    46,    74,    12,    11,    81,   309,   165,\n",
       "           228,  1083,    15,    23,   266,    11,  1999]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0][0], b[0][1][0], b[0][2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the categorical, continuous, and token ids for the first item in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TABULAR:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>business_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>business_postal_code</th>\n",
       "      <th>business_state</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_average_stars</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>business_review_count</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>JFaTPxWQC14VmFnXLv7W1g</td>\n",
       "      <td>NjBbyIIgpN8lHolFh4nUOw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44107</td>\n",
       "      <td>OH</td>\n",
       "      <td>-0.0993</td>\n",
       "      <td>-0.7070</td>\n",
       "      <td>-0.2977</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>vG7ID9YnW3NcEEcwUfipiQ</td>\n",
       "      <td>LVVJqHwWEVCIYbHLILKdQg</td>\n",
       "      <td>4.5</td>\n",
       "      <td>44011</td>\n",
       "      <td>OH</td>\n",
       "      <td>2.2789</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>-0.3206</td>\n",
       "      <td>-0.2633</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>UiZYmFtORXn_l23FgbcPlQ</td>\n",
       "      <td>EpkuViUvQo_5DwZEwj-xKg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>85085</td>\n",
       "      <td>AZ</td>\n",
       "      <td>-0.3635</td>\n",
       "      <td>-0.9182</td>\n",
       "      <td>-0.3406</td>\n",
       "      <td>-0.4529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ad0LjmuXhUGBxTSkve5LZA</td>\n",
       "      <td>maqOMHj4zlKA-lel6nm4RA</td>\n",
       "      <td>4.5</td>\n",
       "      <td>85282</td>\n",
       "      <td>AZ</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>-0.2806</td>\n",
       "      <td>-0.2417</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>QX5Y_BIxlMG3B3vV1zx1Gw</td>\n",
       "      <td>UKmzVr82ihW-c4MIgA_kkQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>M5A 3L3</td>\n",
       "      <td>ON</td>\n",
       "      <td>-0.0993</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>-0.2435</td>\n",
       "      <td>-0.3269</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TEXT:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text_data</th>      <th>target</th>    </tr>\n",
       "  </thead>\n",
       "  <tbody>  <tbody>    <tr>      <td>xxbos(2) xxmaj(5) just(63) visited(897) again(137) 2(152) days(439) ago(516) .(9) xxmaj(5) how(144) this(28) place(44) does(232) n't(33) have(35) 5(146) stars(261) from(71) every(183) reviewer(4683) is(19) beyond(739) me(49) .(9)</td>      <td>5</td>    </tr>    <tr>      <td>xxbos(2) xxmaj(5) my(23) wife(502) and(11) i(13) had(36) never(125) had(36) xxmaj(5) turkish(5498) food(42) before(159) this(28) and(11) were(39) somewhat(1403) apprehensive(6562) but(31) decided(341) to(15) give(187) a(14) shot(1138) .(9) xxmaj(5) we(26) 're(167) glad(670) we(26) did(70) because(101) it(17) 's(37) delicious(145) !(20) i(13) had(36) a(14) dish(336) called(267) xxmaj(5) xxunk(0) xxmaj(5) special(302) with(30) lamb(953) ,(12) vegetables(1119) ,(12) and(11) a(14) somewhat(1403) spicy(424) sauce(197) surrounded(4202) by(99) an(78) eggplant(1959) puree(4382) .(9) xxmaj(5) the(10) bread(338) is(19) also(92) excellent(243) ((54) tastes(917) kind(306) of(18) like(60) pizza(203) crust(890) )(52) and(11) comes(447) with(30) two(156) different(250) dipping(1778) sauces(992) .(9) xxmaj(5) the(10) server(263) was(16) very(50) attentive(544) and(11) kept(594) refilling(5180) our(62) tea(379) and(11) bringing(1866) more(94) bread(338) .(9) xxmaj(5) serving(1016) sizes(1935) are(41) huge(321) .(9) i(13) think(171) i(13) needed(396) to(15) be(46) rolled(2328) out(57) of(18) there(48) !(20) xxmaj(5) definitely(126) no(86) room(199) for(21) desert(1485) .(9)</td>      <td>5</td>    </tr>    <tr>      <td>xxbos(2) xxmaj(5) standing(1415) out(57) here(58) right(162) now(173) 15(451) minutes(174) past(609) opening(1361) time(64) while(172) the(10) associates(3303) are(41) just(63) looking(227) at(40) me(49) like(60) i(13) 'm(117) a(14) nuisance(8184) to(15) them(97) .(9) xxmaj(5) they(29) just(63) lost(999) my(23) business(326) .(9) i(13) 'll(217) go(77) next(202) door(463) to(15) xxunk(0) .(9)</td>      <td>1</td>    </tr>    <tr>      <td>xxbos(2) i(13) hope(795) they(29) realize(1562) how(144) hard(359) it(17) was(16) for(21) me(49) to(15) get(68) here(58) to(15) eat(191) .(9) xxmaj(5) between(569) my(23) girlfriend(1334) feeling(804) unsure(4069) and(11) my(23) friend(300) who(149) is(19) n't(33) into(223) the(10) cuisine(1691) ,(12) it(17) took(194) a(14) lot(216) of(18) xxunk(0) and(11) evil(7681) planning(1995) to(15) finally(423) have(35) a(14) taste(277) .(9) i(13) was(16) not(34) disappointed(361) in(22) the(10) slightest(8928) and(11) was(16) plenty(625) happy(234) with(30) the(10) service(61) .(9) i(13) had(36) wish(507) i(13) got(96) the(10) cashier(1195) 's(37) name(476) ,(12) but(31) he(76) felt(385) like(60) someone(375) you(27) could(109) just(63) hang(1318) out(57) with(30) if(56) he(76) was(16) n't(33) working(466) .(9) xxmaj(5) it(17) was(16) my(23) first(120) time(64) there(48) and(11) i(13) felt(385) like(60) i(13) 'd(274) been(85) eating(446) there(48) for(21) ages(3385) !(20) xxmaj(5) looking(227) at(40) the(10) pictures(1258) of(18) the(10) food(42) ,(12) i(13) had(36) no(86) doubt(1511) that(24) i(13) would(66) love(115) this(28) place(44) already(500) .(9) i(13) had(36) such(355) confidence(3889) that(24) i(13) signed(2024) up(73) for(21) their(69) membership(2575) plan(966) already(500) .(9) xxmaj(5) me(49) and(11) my(23) girlfriend(1334) both(228) ordered(111) the(10) chicken(130) schnitzel(4950) cordon(10126) bleu(5919) and(11) enjoyed(370) every(183) bit(220) of(18) it(17) .(9) xxmaj(5) the(10) humor(3719) of(18) it(17) all(53) is(19) that(24) she(83) hates(5270) mushrooms(1180) ,(12) but(31) loved(352) the(10) gravy(1436) .(9) xxmaj(5) now(173) ,(12) despite(1109) my(23) rave(2120) for(21) the(10) place(44) ,(12) i(13) do(59) have(35) a(14) couple(395) minor(2355) xxunk(0) .(9) ((54) a(14) brave(6507) thing(252) after(105) just(63) giving(759) them(97) my(23) phone(418) number(742) and(11) full(289) name(476) )(52) .(9) i(13) ordered(111) some(90) poutine(1937) as(47) an(78) appetizer(710) and(11) we(26) both(228) liked(545) it(17) ((54) her(128) more(94) than(122) me(49) )(52) .(9) i(13) think(171) the(10) fries(246) should(245) have(35) had(36) more(94) of(18) a(14) crunch(2711) when(72) they(29) came(118) out(57) .(9) xxmaj(5) before(159) i(13) 'm(117) beaten(3914) with(30) a(14) tire(2580) iron(3419) because(101) the(10) fries(246) are(41) coated(4995) in(22) gravy(1436) ,(12) just(63) hear(730) me(49) out(57) .(9) i(13) took(194) a(14) bite(711) off(142) a(14) piece(875) that(24) had(36) no(86) gravy(1436) ,(12) so(38) i(13) 'm(117) not(34) a(14) complete(1046) idiot(4427) here(58) .(9) xxmaj(5) like(60) someone(375) else(327) posted(2005) ,(12) the(10) canned(3061) green(575) beans(758) were(39) n't(33) the(10) best(106) ,(12) but(31) the(10) spicing(13083) did(70) help(390) it(17) quite(322) a(14) bit(220) .(9) i(13) blame(3833) my(23) history(2590) of(18) eating(446) green(575) beans(758) out(57) of(18) a(14) can(89) when(72) i(13) was(16) a(14) kid(1264) so(38) it(17) was(16) a(14) little(132) off(142) putting(1593) for(21) me(49) .(9) i(13) give(187) it(17) a(14) 4(119) 1(268) /(112) 2(152) out(57) of(18) 5(146) and(11) i(13) look(283) forward(819) to(15) eating(446) here(58) again(137) .(9) xxmaj(5) got(96) ta(1877) go(77) ,(12) i(13) got(96) a(14) beaver(13084) ball(1906) with(30) my(23) name(476) on(32) it(17) !(20)</td>      <td>5</td>    </tr>    <tr>      <td>xxbos(2) xxmaj(5) cons(2578) :(141) xxmaj(5) no(86) table(201) service(61) xxmaj(5) pros(2271) :(141) xxmaj(5) summer(1055) patio(541) is(19) awesome(225) when(72) open(388) ,(12) great(51) music(482) ,(12) the(10) shots(2385) out(57) of(18) mason(6038) jars(8303) are(41) really(80) charming(2803)</td>      <td>4</td>    </tr>  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_bunch.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we included the `Normalize` proc, notice that the continuous variables are normalized *per dataset*: \n",
    "`(x - x.mean) / x.std`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
